{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Microsoft (MSFT) Cleaning and Exploration\n",
        "\n",
        "This notebook downloads the specified Kaggle dataset, applies a repeatable cleaning pipeline, generates diagnostic visualizations, and exports the processed data as a CSV file ready for AI forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sofnb25/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_id: str) -> Path:\n",
        "    \"\"\"Download a Kaggle dataset using kagglehub and return the download directory.\"\"\"\n",
        "    download_path = Path(kagglehub.dataset_download(dataset_id))\n",
        "    print(f\"Dataset downloaded to: {download_path}\")\n",
        "    return download_path\n",
        "\n",
        "def select_csv_file(download_path: Path, search_terms: list[str]) -> Path:\n",
        "    \"\"\"Select the most relevant CSV file inside the dataset directory.\"\"\"\n",
        "    csv_files = sorted(download_path.rglob(\"*.csv\"))\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(\"No CSV files were found in the downloaded dataset directory.\")\n",
        "\n",
        "    search_terms_lower = [term.lower() for term in search_terms]\n",
        "    for csv_file in csv_files:\n",
        "        filename = csv_file.name.lower()\n",
        "        if any(term in filename for term in search_terms_lower):\n",
        "            print(f\"Selected CSV file: {csv_file}\")\n",
        "            return csv_file\n",
        "\n",
        "    print(\"No CSV file matched the search terms; using the first CSV found.\")\n",
        "    return csv_files[0]\n",
        "\n",
        "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Rename common financial columns to a consistent schema.\"\"\"\n",
        "    column_mapping = {\n",
        "        \"date\": \"date\",\n",
        "        \"datetime\": \"date\",\n",
        "        \"timestamp\": \"date\",\n",
        "        \"close\": \"close\",\n",
        "        \"close/last\": \"close\",\n",
        "        \"closing price\": \"close\",\n",
        "        \"adj close\": \"close\",\n",
        "        \"adjusted close\": \"close\",\n",
        "        \"close price\": \"close\",\n",
        "        \"close*\": \"close\",\n",
        "        \"open\": \"open\",\n",
        "        \"open/last\": \"open\",\n",
        "        \"high\": \"high\",\n",
        "        \"low\": \"low\",\n",
        "        \"volume\": \"volume\",\n",
        "    }\n",
        "\n",
        "    rename_dict: dict[str, str] = {}\n",
        "    for column in df.columns:\n",
        "        normalized = column.strip().lower()\n",
        "        if normalized in column_mapping:\n",
        "            rename_dict[column] = column_mapping[normalized]\n",
        "\n",
        "    df = df.rename(columns=rename_dict)\n",
        "\n",
        "    if \"close\" not in df.columns:\n",
        "        raise KeyError(\"The dataset does not contain a recognizable close price column.\")\n",
        "\n",
        "    if \"date\" not in df.columns:\n",
        "        raise KeyError(\"The dataset does not contain a recognizable date column.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def coerce_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Convert string-based numeric columns to floats.\"\"\"\n",
        "    numeric_candidates = [col for col in df.columns if col != \"date\"]\n",
        "\n",
        "    for column in numeric_candidates:\n",
        "        df[column] = (\n",
        "            df[column]\n",
        "            .astype(str)\n",
        "            .str.replace(\",\", \"\", regex=False)\n",
        "            .str.replace(\"$\", \"\", regex=False)\n",
        "            .str.replace(\"%\", \"\", regex=False)\n",
        "        )\n",
        "        df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_dataframe(df: pd.DataFrame, start_date: str | None, end_date: str | None) -> pd.DataFrame:\n",
        "    \"\"\"Apply chronological sorting, type conversion, filtering, and deduplication.\"\"\"\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\", \"close\"])\n",
        "    df = df.sort_values(\"date\")\n",
        "    df = df.drop_duplicates(subset=\"date\", keep=\"last\")\n",
        "\n",
        "    df = coerce_numeric_columns(df)\n",
        "\n",
        "    if start_date:\n",
        "        df = df[df[\"date\"] >= pd.to_datetime(start_date)]\n",
        "    if end_date:\n",
        "        df = df[df[\"date\"] <= pd.to_datetime(end_date)]\n",
        "\n",
        "    df = df.dropna(subset=[\"close\"])\n",
        "\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def export_dataframe(df: pd.DataFrame, output_path: Path) -> None:\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Clean dataset saved to: {output_path}\")\n",
        "\n",
        "def summarize_dataframe(df: pd.DataFrame) -> dict[str, float]:\n",
        "    summary = {\n",
        "        \"start_date\": df[\"date\"].min().date().isoformat() if not df.empty else None,\n",
        "        \"end_date\": df[\"date\"].max().date().isoformat() if not df.empty else None,\n",
        "        \"row_count\": int(len(df)),\n",
        "        \"missing_close\": int(df[\"close\"].isna().sum()),\n",
        "    }\n",
        "    print(json.dumps(summary, indent=2))\n",
        "    return summary\n",
        "\n",
        "def plot_dataset(df: pd.DataFrame, title: str) -> None:\n",
        "    if df.empty:\n",
        "        print(\"No data available to plot.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(df[\"date\"], df[\"close\"], label=\"Close Price\", color=\"royalblue\")\n",
        "    plt.title(f\"{title} — Closing Price Over Time\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    df.boxplot(column=\"close\", by=\"year\", grid=False)\n",
        "    plt.title(f\"{title} — Distribution of Close Price by Year\")\n",
        "    plt.suptitle(\"\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    df.drop(columns=\"year\", inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'null' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m OUTPUT_PATH = DATA_DIR / \u001b[33m\"\u001b[39m\u001b[33mmsft_cleaned.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m START_DATE = \u001b[33m\"\u001b[39m\u001b[33m2018-01-01\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m END_DATE = \u001b[43mnull\u001b[49m\n",
            "\u001b[31mNameError\u001b[39m: name 'null' is not defined"
          ]
        }
      ],
      "source": [
        "DATASET_ID = \"adilshamim8/microsoft-stock-price-history\"\n",
        "SEARCH_TERMS = [\"msft\", \"microsoft\"]\n",
        "OUTPUT_PATH = DATA_DIR / \"msft_cleaned.csv\"\n",
        "START_DATE = \"2018-01-01\"\n",
        "END_DATE = null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "download_path = download_dataset(DATASET_ID)\n",
        "raw_csv_path = select_csv_file(download_path, SEARCH_TERMS)\n",
        "raw_df = pd.read_csv(raw_csv_path)\n",
        "\n",
        "standardized_df = standardize_columns(raw_df)\n",
        "clean_df = clean_dataframe(standardized_df, start_date=START_DATE, end_date=END_DATE)\n",
        "summary = summarize_dataframe(clean_df)\n",
        "\n",
        "plot_dataset(clean_df, title=f\"{SEARCH_TERMS[0].upper()} Dataset\")\n",
        "export_dataframe(clean_df, OUTPUT_PATH)\n",
        "clean_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
