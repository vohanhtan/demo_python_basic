{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon (AMZN) Cleaning and Exploration\n",
        "\n",
        "This notebook downloads the specified Kaggle dataset, applies a repeatable cleaning pipeline, generates diagnostic visualizations, and exports the processed data as a CSV file ready for AI forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_id: str) -> Path:\n",
        "    \"\"\"Download a Kaggle dataset using kagglehub and return the download directory.\"\"\"\n",
        "    download_path = Path(kagglehub.dataset_download(dataset_id))\n",
        "    print(f\"Dataset downloaded to: {download_path}\")\n",
        "    return download_path\n",
        "\n",
        "def select_csv_file(download_path: Path, search_terms: list[str]) -> Path:\n",
        "    \"\"\"Select the most relevant CSV file inside the dataset directory.\"\"\"\n",
        "    csv_files = sorted(download_path.rglob(\"*.csv\"))\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(\"No CSV files were found in the downloaded dataset directory.\")\n",
        "\n",
        "    search_terms_lower = [term.lower() for term in search_terms]\n",
        "    for csv_file in csv_files:\n",
        "        filename = csv_file.name.lower()\n",
        "        if any(term in filename for term in search_terms_lower):\n",
        "            print(f\"Selected CSV file: {csv_file}\")\n",
        "            return csv_file\n",
        "\n",
        "    print(\"No CSV file matched the search terms; using the first CSV found.\")\n",
        "    return csv_files[0]\n",
        "\n",
        "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Rename common financial columns to a consistent schema.\"\"\"\n",
        "    column_mapping = {\n",
        "        \"date\": \"date\",\n",
        "        \"datetime\": \"date\",\n",
        "        \"timestamp\": \"date\",\n",
        "        \"close\": \"close\",\n",
        "        \"close/last\": \"close\",\n",
        "        \"closing price\": \"close\",\n",
        "        \"adj close\": \"close\",\n",
        "        \"adjusted close\": \"close\",\n",
        "        \"close price\": \"close\",\n",
        "        \"close*\": \"close\",\n",
        "        \"open\": \"open\",\n",
        "        \"open/last\": \"open\",\n",
        "        \"high\": \"high\",\n",
        "        \"low\": \"low\",\n",
        "        \"volume\": \"volume\",\n",
        "    }\n",
        "\n",
        "    rename_dict: dict[str, str] = {}\n",
        "    for column in df.columns:\n",
        "        normalized = column.strip().lower()\n",
        "        if normalized in column_mapping:\n",
        "            rename_dict[column] = column_mapping[normalized]\n",
        "\n",
        "    df = df.rename(columns=rename_dict)\n",
        "\n",
        "    if \"close\" not in df.columns:\n",
        "        raise KeyError(\"The dataset does not contain a recognizable close price column.\")\n",
        "\n",
        "    if \"date\" not in df.columns:\n",
        "        raise KeyError(\"The dataset does not contain a recognizable date column.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def coerce_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Convert string-based numeric columns to floats.\"\"\"\n",
        "    numeric_candidates = [col for col in df.columns if col != \"date\"]\n",
        "\n",
        "    for column in numeric_candidates:\n",
        "        df[column] = (\n",
        "            df[column]\n",
        "            .astype(str)\n",
        "            .str.replace(\",\", \"\", regex=False)\n",
        "            .str.replace(\"$\", \"\", regex=False)\n",
        "            .str.replace(\"%\", \"\", regex=False)\n",
        "        )\n",
        "        df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_dataframe(df: pd.DataFrame, start_date: str | None, end_date: str | None) -> pd.DataFrame:\n",
        "    \"\"\"Apply chronological sorting, type conversion, filtering, and deduplication.\"\"\"\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\", \"close\"])\n",
        "    df = df.sort_values(\"date\")\n",
        "    df = df.drop_duplicates(subset=\"date\", keep=\"last\")\n",
        "\n",
        "    df = coerce_numeric_columns(df)\n",
        "\n",
        "    if start_date:\n",
        "        df = df[df[\"date\"] >= pd.to_datetime(start_date)]\n",
        "    if end_date:\n",
        "        df = df[df[\"date\"] <= pd.to_datetime(end_date)]\n",
        "\n",
        "    df = df.dropna(subset=[\"close\"])\n",
        "\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def export_dataframe(df: pd.DataFrame, output_path: Path) -> None:\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Clean dataset saved to: {output_path}\")\n",
        "\n",
        "def summarize_dataframe(df: pd.DataFrame) -> dict[str, float]:\n",
        "    summary = {\n",
        "        \"start_date\": df[\"date\"].min().date().isoformat() if not df.empty else None,\n",
        "        \"end_date\": df[\"date\"].max().date().isoformat() if not df.empty else None,\n",
        "        \"row_count\": int(len(df)),\n",
        "        \"missing_close\": int(df[\"close\"].isna().sum()),\n",
        "    }\n",
        "    print(json.dumps(summary, indent=2))\n",
        "    return summary\n",
        "\n",
        "def plot_dataset(df: pd.DataFrame, title: str) -> None:\n",
        "    if df.empty:\n",
        "        print(\"No data available to plot.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(df[\"date\"], df[\"close\"], label=\"Close Price\", color=\"royalblue\")\n",
        "    plt.title(f\"{title} — Closing Price Over Time\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    df.boxplot(column=\"close\", by=\"year\", grid=False)\n",
        "    plt.title(f\"{title} — Distribution of Close Price by Year\")\n",
        "    plt.suptitle(\"\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Close Price\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    df.drop(columns=\"year\", inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_ID = \"meharshanali/amazon-stocks-2025\"\n",
        "SEARCH_TERMS = [\"amzn\", \"amazon\"]\n",
        "OUTPUT_PATH = DATA_DIR / \"amzn_cleaned.csv\"\n",
        "START_DATE = \"2020-01-01\"\n",
        "END_DATE = \"2025-12-31\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded to: /Users/sofnb25/.cache/kagglehub/datasets/meharshanali/amazon-stocks-2025/versions/1\n",
            "Selected CSV file: /Users/sofnb25/.cache/kagglehub/datasets/meharshanali/amazon-stocks-2025/versions/1/AMZN_stock_data.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3p/54rhmhtx5ssbx40cgtfvc6740000gn/T/ipykernel_7281/2087306193.py:80: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Cannot compare tz-naive and tz-aware timestamps",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m raw_df = pd.read_csv(raw_csv_path)\n\u001b[32m      5\u001b[39m standardized_df = standardize_columns(raw_df)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m clean_df = \u001b[43mclean_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandardized_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTART_DATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m summary = summarize_dataframe(clean_df)\n\u001b[32m      9\u001b[39m plot_dataset(clean_df, title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEARCH_TERMS[\u001b[32m0\u001b[39m].upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Dataset\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mclean_dataframe\u001b[39m\u001b[34m(df, start_date, end_date)\u001b[39m\n\u001b[32m     85\u001b[39m df = coerce_numeric_columns(df)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_date:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     df = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end_date:\n\u001b[32m     90\u001b[39m     df = df[df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m] <= pd.to_datetime(end_date)]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/pandas/core/series.py:6138\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6135\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6136\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6138\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:344\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lvalues.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     res_values = \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MS34/MSA34 - Python/Project/demo_python_basic/.venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:129\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    127\u001b[39m     result = libops.vec_compare(x.ravel(), y.ravel(), op)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     result = \u001b[43mlibops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(x.shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/ops.pyx:107\u001b[39m, in \u001b[36mpandas._libs.ops.scalar_compare\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/timestamps.pyx:387\u001b[39m, in \u001b[36mpandas._libs.tslibs.timestamps._Timestamp.__richcmp__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: Cannot compare tz-naive and tz-aware timestamps"
          ]
        }
      ],
      "source": [
        "download_path = download_dataset(DATASET_ID)\n",
        "raw_csv_path = select_csv_file(download_path, SEARCH_TERMS)\n",
        "raw_df = pd.read_csv(raw_csv_path)\n",
        "\n",
        "standardized_df = standardize_columns(raw_df)\n",
        "clean_df = clean_dataframe(standardized_df, start_date=START_DATE, end_date=END_DATE)\n",
        "summary = summarize_dataframe(clean_df)\n",
        "\n",
        "plot_dataset(clean_df, title=f\"{SEARCH_TERMS[0].upper()} Dataset\")\n",
        "export_dataframe(clean_df, OUTPUT_PATH)\n",
        "clean_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
